{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing isomap 1...\n",
      "computing isomap 2..\n",
      "ve shape: (3696, 64)\n",
      "adata shape: (3696, 2000)\n",
      "n_components: 100\n",
      "n_neighbors: 10\n",
      "knn rep used: ve\n",
      "ve key used\n",
      "Epoch [1/10], Batch [0], Loss: 0.9556393944869533\n",
      "Epoch [1/10], Batch [10], Loss: 0.8863926671696456\n",
      "Epoch [1/10], Batch [20], Loss: 0.8182053883242436\n",
      "Epoch [1/10], Batch [30], Loss: 0.7568108230413835\n",
      "Epoch [1/10], Batch [40], Loss: 0.6724526597871585\n",
      "Epoch [1/10], Batch [50], Loss: 0.6068205909505628\n",
      "Epoch [1/10], Average Loss: 0.9322851309612108\n",
      "Epoch [2/10], Batch [0], Loss: 0.48643011245546186\n",
      "Epoch [2/10], Batch [10], Loss: 0.4159967876730087\n",
      "Epoch [2/10], Batch [20], Loss: 0.37033921290372457\n",
      "Epoch [2/10], Batch [30], Loss: 0.34180613047067143\n",
      "Epoch [2/10], Batch [40], Loss: 0.3192533184159715\n",
      "Epoch [2/10], Batch [50], Loss: 0.3031333333884139\n",
      "Epoch [2/10], Average Loss: 0.4512835342184697\n",
      "Epoch [3/10], Batch [0], Loss: 0.1447963584555705\n",
      "Epoch [3/10], Batch [10], Loss: 0.16582530783458577\n",
      "Epoch [3/10], Batch [20], Loss: 0.19099219239615584\n",
      "Epoch [3/10], Batch [30], Loss: 0.1583575175643912\n",
      "Epoch [3/10], Batch [40], Loss: 0.18093586143392704\n",
      "Epoch [3/10], Batch [50], Loss: 0.18965738025720386\n",
      "Epoch [3/10], Average Loss: 0.237033158704255\n",
      "Epoch [4/10], Batch [0], Loss: 0.09478999086863824\n",
      "Epoch [4/10], Batch [10], Loss: 0.11566285925908273\n",
      "Epoch [4/10], Batch [20], Loss: 0.1544728596666599\n",
      "Epoch [4/10], Batch [30], Loss: 0.1282044143884546\n",
      "Epoch [4/10], Batch [40], Loss: 0.1364555671607518\n",
      "Epoch [4/10], Batch [50], Loss: 0.15161577668358614\n",
      "Epoch [4/10], Average Loss: 0.18314464494261296\n",
      "Epoch [5/10], Batch [0], Loss: 0.08113689473888044\n",
      "Epoch [5/10], Batch [10], Loss: 0.1003296151620893\n",
      "Epoch [5/10], Batch [20], Loss: 0.14043888886018618\n",
      "Epoch [5/10], Batch [30], Loss: 0.1149666089698799\n",
      "Epoch [5/10], Batch [40], Loss: 0.12385662792071214\n",
      "Epoch [5/10], Batch [50], Loss: 0.13962491513503086\n",
      "Epoch [5/10], Average Loss: 0.16627751714756084\n",
      "Epoch [6/10], Batch [0], Loss: 0.07391257207440524\n",
      "Epoch [6/10], Batch [10], Loss: 0.09128414353457792\n",
      "Epoch [6/10], Batch [20], Loss: 0.13394463938637655\n",
      "Epoch [6/10], Batch [30], Loss: 0.10659210441508526\n",
      "Epoch [6/10], Batch [40], Loss: 0.11659000756113763\n",
      "Epoch [6/10], Batch [50], Loss: 0.13185991235235678\n",
      "Epoch [6/10], Average Loss: 0.15713285159055865\n",
      "Epoch [7/10], Batch [0], Loss: 0.0690490574319565\n",
      "Epoch [7/10], Batch [10], Loss: 0.08531024386816045\n",
      "Epoch [7/10], Batch [20], Loss: 0.12949383958884053\n",
      "Epoch [7/10], Batch [30], Loss: 0.10049197838880487\n",
      "Epoch [7/10], Batch [40], Loss: 0.11221912953321098\n",
      "Epoch [7/10], Batch [50], Loss: 0.12636488536450638\n",
      "Epoch [7/10], Average Loss: 0.150293774764315\n",
      "Epoch [8/10], Batch [0], Loss: 0.06549385160000548\n",
      "Epoch [8/10], Batch [10], Loss: 0.08119890710497207\n",
      "Epoch [8/10], Batch [20], Loss: 0.1256727725385839\n",
      "Epoch [8/10], Batch [30], Loss: 0.0948317409904744\n",
      "Epoch [8/10], Batch [40], Loss: 0.1084323511691331\n",
      "Epoch [8/10], Batch [50], Loss: 0.12054777842911107\n",
      "Epoch [8/10], Average Loss: 0.14445281634550602\n",
      "Epoch [9/10], Batch [0], Loss: 0.06280893192341808\n",
      "Epoch [9/10], Batch [10], Loss: 0.0769048248650456\n",
      "Epoch [9/10], Batch [20], Loss: 0.12244831826528477\n",
      "Epoch [9/10], Batch [30], Loss: 0.08894694769232865\n",
      "Epoch [9/10], Batch [40], Loss: 0.10455080036759556\n",
      "Epoch [9/10], Batch [50], Loss: 0.11498910906892995\n",
      "Epoch [9/10], Average Loss: 0.1389439103754141\n",
      "Epoch [10/10], Batch [0], Loss: 0.060240354382136464\n",
      "Epoch [10/10], Batch [10], Loss: 0.07257969419438096\n",
      "Epoch [10/10], Batch [20], Loss: 0.11924234700586539\n",
      "Epoch [10/10], Batch [30], Loss: 0.08350704866822876\n",
      "Epoch [10/10], Batch [40], Loss: 0.10090753141234407\n",
      "Epoch [10/10], Batch [50], Loss: 0.1098787476937722\n",
      "Epoch [10/10], Average Loss: 0.13351215480584971\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "from model import NETWORK  # Ensure that model.py is saved in the same directory\n",
    "from dataloaders import * # Ensure that dataloaders.py is saved in the same directory\n",
    "from utils import *\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "\n",
    "# Setup configuration\n",
    "latent_dim = 64  # Latent dimension size, can be adjusted\n",
    "hidden_dim = 512  # Hidden dimension size for the encoder and decoder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "n_components = 100\n",
    "n_knn_search = 10\n",
    "dataset_name = \"pancreas\"\n",
    "cell_type_key = \"clusters\"\n",
    "model_name = \"VeloFormer\"\n",
    "\n",
    "num_genes = 2000\n",
    "nhead = 1 #original: 1\n",
    "embedding_dim = 128*nhead# original: 128\n",
    "num_encoder_layers = 1 #original: 1\n",
    "num_bins = 50\n",
    "batch_size = 64  # Batch size for training\n",
    "epochs = 10 # Number of epochs for training\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "lambda1 = 1 # Weight for heuristic loss\n",
    "lambda2 = 0 # Weight for discrepancy loss\n",
    "K = 11  # Number of neighbors for heuristic loss\n",
    "\n",
    "\n",
    "knn_rep = \"ve\"\n",
    "best_key = None\n",
    "ve_layer = \"None\"\n",
    "\n",
    "# Load data\n",
    "adata = sc.read_h5ad(\"pancreas-gastr->pancr_transfer_isomap_latest.h5ad\")\n",
    "#adata.obsm[\"MuMs\"] = np.concatenate([adata.layers[\"Ms\"]], axis=1)\n",
    "#manifold_and_neighbors(adata, n_components, n_knn_search, dataset_name, K, knn_rep, best_key, ve_layer)\n",
    "adata = color_keys(adata, cell_type_key)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = NETWORK(input_dim=num_genes*2, latent_dim=latent_dim, \n",
    "                hidden_dim=hidden_dim, emb_dim = embedding_dim,\n",
    "                nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "                num_genes=num_genes, num_bins=num_bins).to(device)\n",
    "                \n",
    "model.load_state_dict(torch.load('model_10epochs.pth'))\n",
    "\n",
    "# Reinitialize weights of the derivative_decoder and probabilities_decoder\n",
    "def reinitialize_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)         # Initialize biases to zero\n",
    "\n",
    "# Apply reinitialization to specific decoders\n",
    "#model.derivative_decoder.apply(reinitialize_weights)\n",
    "#model.probabilities_decoder.apply(reinitialize_weights)\n",
    "\n",
    "\n",
    "# Freeze all layers except for the derivative and probabilities decoders\n",
    "for name, param in model.named_parameters():\n",
    "    if \"derivative_decoder\" not in name and \"probabilities_decoder\" not in name:\n",
    "        param.requires_grad = False  # Freeze the parameters\n",
    "    else:\n",
    "        param.requires_grad = True   # Keep the decoders' parameters trainable\n",
    "\n",
    "\n",
    "# Ensure optimizer only updates trainable parameters\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader, test_loader, full_data_loader = setup_dataloaders_binning_simpler(adata, \n",
    "                                                                       batch_size=batch_size, \n",
    "                                                                       num_genes=num_genes,\n",
    "                                                                       num_bins=num_bins)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (tokens, data, batch_indices) in enumerate(full_data_loader):\n",
    "        tokens = tokens.to(device)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"\"\"print(tokens.shape)\n",
    "        print(data.shape)\n",
    "        print(batch_indices.shape)\"\"\"\n",
    "        \n",
    "        # Forward pass\n",
    "        out_dic = model(tokens, data)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses_dic = model.heuristic_loss(\n",
    "            adata=adata, \n",
    "            x=data, \n",
    "            batch_indices=batch_indices,\n",
    "            lambda1=lambda1, \n",
    "            lambda2=lambda2, \n",
    "            out_dic=out_dic, \n",
    "            device=device,\n",
    "            K=K\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss = losses_dic[\"total_loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for monitoring\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}], Loss: {loss.item()}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Save the model periodically\n",
    "    \"\"\"if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\"\"\"\n",
    "\n",
    "# After training, save final model\n",
    "torch.save(model.state_dict(), 'linear_probed_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTrajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
