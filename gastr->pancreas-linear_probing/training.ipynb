{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Batch [0], Loss: 0.09864248335361481\n",
      "Epoch [1/40], Batch [10], Loss: 0.07544733583927155\n",
      "Epoch [1/40], Batch [20], Loss: 0.05124034732580185\n",
      "Epoch [1/40], Batch [30], Loss: 0.03688408434391022\n",
      "Epoch [1/40], Batch [40], Loss: 0.03553071245551109\n",
      "Epoch [1/40], Batch [50], Loss: 0.026886776089668274\n",
      "Epoch [1/40], Average Loss: 0.06059507700674077\n",
      "Epoch [2/40], Batch [0], Loss: 0.028813861310482025\n",
      "Epoch [2/40], Batch [10], Loss: 0.028120482340455055\n",
      "Epoch [2/40], Batch [20], Loss: 0.02355247177183628\n",
      "Epoch [2/40], Batch [30], Loss: 0.020479954779148102\n",
      "Epoch [2/40], Batch [40], Loss: 0.023240379989147186\n",
      "Epoch [2/40], Batch [50], Loss: 0.01953015848994255\n",
      "Epoch [2/40], Average Loss: 0.028477734093494873\n",
      "Epoch [3/40], Batch [0], Loss: 0.02390800043940544\n",
      "Epoch [3/40], Batch [10], Loss: 0.02201835811138153\n",
      "Epoch [3/40], Batch [20], Loss: 0.019641941413283348\n",
      "Epoch [3/40], Batch [30], Loss: 0.01574295200407505\n",
      "Epoch [3/40], Batch [40], Loss: 0.018812529742717743\n",
      "Epoch [3/40], Batch [50], Loss: 0.016947761178016663\n",
      "Epoch [3/40], Average Loss: 0.02297923373098069\n",
      "Epoch [4/40], Batch [0], Loss: 0.021717313677072525\n",
      "Epoch [4/40], Batch [10], Loss: 0.019058125093579292\n",
      "Epoch [4/40], Batch [20], Loss: 0.01738261617720127\n",
      "Epoch [4/40], Batch [30], Loss: 0.013520128093659878\n",
      "Epoch [4/40], Batch [40], Loss: 0.016825469210743904\n",
      "Epoch [4/40], Batch [50], Loss: 0.015662139281630516\n",
      "Epoch [4/40], Average Loss: 0.02024590220064559\n",
      "Epoch [5/40], Batch [0], Loss: 0.020557692274451256\n",
      "Epoch [5/40], Batch [10], Loss: 0.017107605934143066\n",
      "Epoch [5/40], Batch [20], Loss: 0.016212716698646545\n",
      "Epoch [5/40], Batch [30], Loss: 0.012379751540720463\n",
      "Epoch [5/40], Batch [40], Loss: 0.01570846512913704\n",
      "Epoch [5/40], Batch [50], Loss: 0.014735360629856586\n",
      "Epoch [5/40], Average Loss: 0.018533395979791246\n",
      "Epoch [6/40], Batch [0], Loss: 0.01974388025701046\n",
      "Epoch [6/40], Batch [10], Loss: 0.015713850036263466\n",
      "Epoch [6/40], Batch [20], Loss: 0.015153047628700733\n",
      "Epoch [6/40], Batch [30], Loss: 0.011692948639392853\n",
      "Epoch [6/40], Batch [40], Loss: 0.01495062094181776\n",
      "Epoch [6/40], Batch [50], Loss: 0.014040807262063026\n",
      "Epoch [6/40], Average Loss: 0.017285920025661906\n",
      "Epoch [7/40], Batch [0], Loss: 0.0192207433283329\n",
      "Epoch [7/40], Batch [10], Loss: 0.01447420846670866\n",
      "Epoch [7/40], Batch [20], Loss: 0.014150991104543209\n",
      "Epoch [7/40], Batch [30], Loss: 0.011270665563642979\n",
      "Epoch [7/40], Batch [40], Loss: 0.014246025122702122\n",
      "Epoch [7/40], Batch [50], Loss: 0.013269385322928429\n",
      "Epoch [7/40], Average Loss: 0.01629032385832769\n",
      "Epoch [8/40], Batch [0], Loss: 0.01881422847509384\n",
      "Epoch [8/40], Batch [10], Loss: 0.013582469895482063\n",
      "Epoch [8/40], Batch [20], Loss: 0.013373862020671368\n",
      "Epoch [8/40], Batch [30], Loss: 0.010878513567149639\n",
      "Epoch [8/40], Batch [40], Loss: 0.013573678210377693\n",
      "Epoch [8/40], Batch [50], Loss: 0.012578999623656273\n",
      "Epoch [8/40], Average Loss: 0.01545375276436197\n",
      "Epoch [9/40], Batch [0], Loss: 0.01846626028418541\n",
      "Epoch [9/40], Batch [10], Loss: 0.012934624217450619\n",
      "Epoch [9/40], Batch [20], Loss: 0.012770518660545349\n",
      "Epoch [9/40], Batch [30], Loss: 0.010540897957980633\n",
      "Epoch [9/40], Batch [40], Loss: 0.012947374023497105\n",
      "Epoch [9/40], Batch [50], Loss: 0.011924202553927898\n",
      "Epoch [9/40], Average Loss: 0.014737203628062568\n",
      "Epoch [10/40], Batch [0], Loss: 0.018061155453324318\n",
      "Epoch [10/40], Batch [10], Loss: 0.01230682898312807\n",
      "Epoch [10/40], Batch [20], Loss: 0.012201007455587387\n",
      "Epoch [10/40], Batch [30], Loss: 0.010250715538859367\n",
      "Epoch [10/40], Batch [40], Loss: 0.0123875318095088\n",
      "Epoch [10/40], Batch [50], Loss: 0.011331363581120968\n",
      "Epoch [10/40], Average Loss: 0.014079085798894471\n",
      "Epoch [11/40], Batch [0], Loss: 0.017532920464873314\n",
      "Epoch [11/40], Batch [10], Loss: 0.011765820905566216\n",
      "Epoch [11/40], Batch [20], Loss: 0.011760801076889038\n",
      "Epoch [11/40], Batch [30], Loss: 0.009775977581739426\n",
      "Epoch [11/40], Batch [40], Loss: 0.011879988946020603\n",
      "Epoch [11/40], Batch [50], Loss: 0.010660281404852867\n",
      "Epoch [11/40], Average Loss: 0.01344324855450937\n",
      "Epoch [12/40], Batch [0], Loss: 0.01687788963317871\n",
      "Epoch [12/40], Batch [10], Loss: 0.011257125996053219\n",
      "Epoch [12/40], Batch [20], Loss: 0.011326087638735771\n",
      "Epoch [12/40], Batch [30], Loss: 0.009555896744132042\n",
      "Epoch [12/40], Batch [40], Loss: 0.011502585373818874\n",
      "Epoch [12/40], Batch [50], Loss: 0.010065728798508644\n",
      "Epoch [12/40], Average Loss: 0.012845569736066651\n",
      "Epoch [13/40], Batch [0], Loss: 0.01626024767756462\n",
      "Epoch [13/40], Batch [10], Loss: 0.010815329849720001\n",
      "Epoch [13/40], Batch [20], Loss: 0.010953285731375217\n",
      "Epoch [13/40], Batch [30], Loss: 0.009093736298382282\n",
      "Epoch [13/40], Batch [40], Loss: 0.011011547408998013\n",
      "Epoch [13/40], Batch [50], Loss: 0.00961410254240036\n",
      "Epoch [13/40], Average Loss: 0.012281235416439619\n",
      "Epoch [14/40], Batch [0], Loss: 0.015710458159446716\n",
      "Epoch [14/40], Batch [10], Loss: 0.01039525493979454\n",
      "Epoch [14/40], Batch [20], Loss: 0.010671613737940788\n",
      "Epoch [14/40], Batch [30], Loss: 0.008754401467740536\n",
      "Epoch [14/40], Batch [40], Loss: 0.01074519194662571\n",
      "Epoch [14/40], Batch [50], Loss: 0.009227931499481201\n",
      "Epoch [14/40], Average Loss: 0.011781376499207095\n",
      "Epoch [15/40], Batch [0], Loss: 0.015246255323290825\n",
      "Epoch [15/40], Batch [10], Loss: 0.009942199103534222\n",
      "Epoch [15/40], Batch [20], Loss: 0.010350891388952732\n",
      "Epoch [15/40], Batch [30], Loss: 0.00840601697564125\n",
      "Epoch [15/40], Batch [40], Loss: 0.010415513068437576\n",
      "Epoch [15/40], Batch [50], Loss: 0.008918392471969128\n",
      "Epoch [15/40], Average Loss: 0.01133546686949248\n",
      "Epoch [16/40], Batch [0], Loss: 0.014700111001729965\n",
      "Epoch [16/40], Batch [10], Loss: 0.009545719251036644\n",
      "Epoch [16/40], Batch [20], Loss: 0.010006032884120941\n",
      "Epoch [16/40], Batch [30], Loss: 0.008103272877633572\n",
      "Epoch [16/40], Batch [40], Loss: 0.01005672849714756\n",
      "Epoch [16/40], Batch [50], Loss: 0.008626362308859825\n",
      "Epoch [16/40], Average Loss: 0.010913559849909011\n",
      "Epoch [17/40], Batch [0], Loss: 0.014221894554793835\n",
      "Epoch [17/40], Batch [10], Loss: 0.009130368940532207\n",
      "Epoch [17/40], Batch [20], Loss: 0.009668293409049511\n",
      "Epoch [17/40], Batch [30], Loss: 0.007797506637871265\n",
      "Epoch [17/40], Batch [40], Loss: 0.009680437855422497\n",
      "Epoch [17/40], Batch [50], Loss: 0.008357739076018333\n",
      "Epoch [17/40], Average Loss: 0.010509845929497734\n",
      "Epoch [18/40], Batch [0], Loss: 0.013729684054851532\n",
      "Epoch [18/40], Batch [10], Loss: 0.008807582780718803\n",
      "Epoch [18/40], Batch [20], Loss: 0.009343316778540611\n",
      "Epoch [18/40], Batch [30], Loss: 0.007572828326374292\n",
      "Epoch [18/40], Batch [40], Loss: 0.009334749542176723\n",
      "Epoch [18/40], Batch [50], Loss: 0.008028008043766022\n",
      "Epoch [18/40], Average Loss: 0.01012693265294458\n",
      "Epoch [19/40], Batch [0], Loss: 0.013359704986214638\n",
      "Epoch [19/40], Batch [10], Loss: 0.008373104967176914\n",
      "Epoch [19/40], Batch [20], Loss: 0.009045732207596302\n",
      "Epoch [19/40], Batch [30], Loss: 0.00724107725545764\n",
      "Epoch [19/40], Batch [40], Loss: 0.009010928682982922\n",
      "Epoch [19/40], Batch [50], Loss: 0.007746802177280188\n",
      "Epoch [19/40], Average Loss: 0.00976622934908943\n",
      "Epoch [20/40], Batch [0], Loss: 0.012907704338431358\n",
      "Epoch [20/40], Batch [10], Loss: 0.008091863244771957\n",
      "Epoch [20/40], Batch [20], Loss: 0.008823349140584469\n",
      "Epoch [20/40], Batch [30], Loss: 0.006977826356887817\n",
      "Epoch [20/40], Batch [40], Loss: 0.00871344655752182\n",
      "Epoch [20/40], Batch [50], Loss: 0.0074973320588469505\n",
      "Epoch [20/40], Average Loss: 0.009423704629645068\n",
      "Epoch [21/40], Batch [0], Loss: 0.012635248713195324\n",
      "Epoch [21/40], Batch [10], Loss: 0.007834836840629578\n",
      "Epoch [21/40], Batch [20], Loss: 0.008501574397087097\n",
      "Epoch [21/40], Batch [30], Loss: 0.0066999937407672405\n",
      "Epoch [21/40], Batch [40], Loss: 0.008394584991037846\n",
      "Epoch [21/40], Batch [50], Loss: 0.007213097531348467\n",
      "Epoch [21/40], Average Loss: 0.009101732861884732\n",
      "Epoch [22/40], Batch [0], Loss: 0.012219484895467758\n",
      "Epoch [22/40], Batch [10], Loss: 0.007623825687915087\n",
      "Epoch [22/40], Batch [20], Loss: 0.008222978562116623\n",
      "Epoch [22/40], Batch [30], Loss: 0.006503970827907324\n",
      "Epoch [22/40], Batch [40], Loss: 0.008081825450062752\n",
      "Epoch [22/40], Batch [50], Loss: 0.007022363133728504\n",
      "Epoch [22/40], Average Loss: 0.008807672910947115\n",
      "Epoch [23/40], Batch [0], Loss: 0.011981161311268806\n",
      "Epoch [23/40], Batch [10], Loss: 0.007295109797269106\n",
      "Epoch [23/40], Batch [20], Loss: 0.007807986810803413\n",
      "Epoch [23/40], Batch [30], Loss: 0.006283460184931755\n",
      "Epoch [23/40], Batch [40], Loss: 0.007792129646986723\n",
      "Epoch [23/40], Batch [50], Loss: 0.006810742896050215\n",
      "Epoch [23/40], Average Loss: 0.008516628978813582\n",
      "Epoch [24/40], Batch [0], Loss: 0.011682084761559963\n",
      "Epoch [24/40], Batch [10], Loss: 0.007056326139718294\n",
      "Epoch [24/40], Batch [20], Loss: 0.0075366804376244545\n",
      "Epoch [24/40], Batch [30], Loss: 0.006025675218552351\n",
      "Epoch [24/40], Batch [40], Loss: 0.0075034284964203835\n",
      "Epoch [24/40], Batch [50], Loss: 0.006612458266317844\n",
      "Epoch [24/40], Average Loss: 0.008239443166221076\n",
      "Epoch [25/40], Batch [0], Loss: 0.01137517113238573\n",
      "Epoch [25/40], Batch [10], Loss: 0.006850273348391056\n",
      "Epoch [25/40], Batch [20], Loss: 0.007270192727446556\n",
      "Epoch [25/40], Batch [30], Loss: 0.005911150015890598\n",
      "Epoch [25/40], Batch [40], Loss: 0.007305159233510494\n",
      "Epoch [25/40], Batch [50], Loss: 0.006459782365709543\n",
      "Epoch [25/40], Average Loss: 0.00797860382838135\n",
      "Epoch [26/40], Batch [0], Loss: 0.011116265319287777\n",
      "Epoch [26/40], Batch [10], Loss: 0.006702410988509655\n",
      "Epoch [26/40], Batch [20], Loss: 0.007034055422991514\n",
      "Epoch [26/40], Batch [30], Loss: 0.005674748681485653\n",
      "Epoch [26/40], Batch [40], Loss: 0.006988840643316507\n",
      "Epoch [26/40], Batch [50], Loss: 0.006273983977735043\n",
      "Epoch [26/40], Average Loss: 0.007742750577311566\n",
      "Epoch [27/40], Batch [0], Loss: 0.010889722965657711\n",
      "Epoch [27/40], Batch [10], Loss: 0.0064456588588654995\n",
      "Epoch [27/40], Batch [20], Loss: 0.006821602117270231\n",
      "Epoch [27/40], Batch [30], Loss: 0.0054639545269310474\n",
      "Epoch [27/40], Batch [40], Loss: 0.006807707250118256\n",
      "Epoch [27/40], Batch [50], Loss: 0.006157770287245512\n",
      "Epoch [27/40], Average Loss: 0.007517084261362857\n",
      "Epoch [28/40], Batch [0], Loss: 0.010730961337685585\n",
      "Epoch [28/40], Batch [10], Loss: 0.006391032133251429\n",
      "Epoch [28/40], Batch [20], Loss: 0.006679967045783997\n",
      "Epoch [28/40], Batch [30], Loss: 0.005340972449630499\n",
      "Epoch [28/40], Batch [40], Loss: 0.006685537286102772\n",
      "Epoch [28/40], Batch [50], Loss: 0.006044806446880102\n",
      "Epoch [28/40], Average Loss: 0.00733807222283584\n",
      "Epoch [29/40], Batch [0], Loss: 0.010517139919102192\n",
      "Epoch [29/40], Batch [10], Loss: 0.00632934644818306\n",
      "Epoch [29/40], Batch [20], Loss: 0.006519060581922531\n",
      "Epoch [29/40], Batch [30], Loss: 0.005241525359451771\n",
      "Epoch [29/40], Batch [40], Loss: 0.006494079250842333\n",
      "Epoch [29/40], Batch [50], Loss: 0.005948382429778576\n",
      "Epoch [29/40], Average Loss: 0.00719476941021833\n",
      "Epoch [30/40], Batch [0], Loss: 0.010279154404997826\n",
      "Epoch [30/40], Batch [10], Loss: 0.00613089744001627\n",
      "Epoch [30/40], Batch [20], Loss: 0.0063681392930448055\n",
      "Epoch [30/40], Batch [30], Loss: 0.00508379191160202\n",
      "Epoch [30/40], Batch [40], Loss: 0.006323715206235647\n",
      "Epoch [30/40], Batch [50], Loss: 0.0058884029276669025\n",
      "Epoch [30/40], Average Loss: 0.007019910733512742\n",
      "Epoch [31/40], Batch [0], Loss: 0.010186816565692425\n",
      "Epoch [31/40], Batch [10], Loss: 0.0060156104154884815\n",
      "Epoch [31/40], Batch [20], Loss: 0.006263705436140299\n",
      "Epoch [31/40], Batch [30], Loss: 0.004954617004841566\n",
      "Epoch [31/40], Batch [40], Loss: 0.006308209151029587\n",
      "Epoch [31/40], Batch [50], Loss: 0.006006244104355574\n",
      "Epoch [31/40], Average Loss: 0.0069173406809568405\n",
      "Epoch [32/40], Batch [0], Loss: 0.010025891475379467\n",
      "Epoch [32/40], Batch [10], Loss: 0.006406289525330067\n",
      "Epoch [32/40], Batch [20], Loss: 0.006168769672513008\n",
      "Epoch [32/40], Batch [30], Loss: 0.004852654878050089\n",
      "Epoch [32/40], Batch [40], Loss: 0.006113827228546143\n",
      "Epoch [32/40], Batch [50], Loss: 0.005823517683893442\n",
      "Epoch [32/40], Average Loss: 0.006832207473827169\n",
      "Epoch [33/40], Batch [0], Loss: 0.009751589968800545\n",
      "Epoch [33/40], Batch [10], Loss: 0.005954877007752657\n",
      "Epoch [33/40], Batch [20], Loss: 0.006054368335753679\n",
      "Epoch [33/40], Batch [30], Loss: 0.0047831605188548565\n",
      "Epoch [33/40], Batch [40], Loss: 0.005973398685455322\n",
      "Epoch [33/40], Batch [50], Loss: 0.005706970579922199\n",
      "Epoch [33/40], Average Loss: 0.00670476472797863\n",
      "Epoch [34/40], Batch [0], Loss: 0.009558235295116901\n",
      "Epoch [34/40], Batch [10], Loss: 0.0058203632943332195\n",
      "Epoch [34/40], Batch [20], Loss: 0.0059715500101447105\n",
      "Epoch [34/40], Batch [30], Loss: 0.004659383092075586\n",
      "Epoch [34/40], Batch [40], Loss: 0.005902398377656937\n",
      "Epoch [34/40], Batch [50], Loss: 0.005635513924062252\n",
      "Epoch [34/40], Average Loss: 0.006559180110653347\n",
      "Epoch [35/40], Batch [0], Loss: 0.009357963688671589\n",
      "Epoch [35/40], Batch [10], Loss: 0.005708854179829359\n",
      "Epoch [35/40], Batch [20], Loss: 0.005881894379854202\n",
      "Epoch [35/40], Batch [30], Loss: 0.00460698502138257\n",
      "Epoch [35/40], Batch [40], Loss: 0.005666144657880068\n",
      "Epoch [35/40], Batch [50], Loss: 0.005561872851103544\n",
      "Epoch [35/40], Average Loss: 0.006413947092369199\n",
      "Epoch [36/40], Batch [0], Loss: 0.009200561791658401\n",
      "Epoch [36/40], Batch [10], Loss: 0.005632201675325632\n",
      "Epoch [36/40], Batch [20], Loss: 0.005721393506973982\n",
      "Epoch [36/40], Batch [30], Loss: 0.0046171690337359905\n",
      "Epoch [36/40], Batch [40], Loss: 0.005700237117707729\n",
      "Epoch [36/40], Batch [50], Loss: 0.0054712556302547455\n",
      "Epoch [36/40], Average Loss: 0.006309752312904977\n",
      "Epoch [37/40], Batch [0], Loss: 0.009055111557245255\n",
      "Epoch [37/40], Batch [10], Loss: 0.005663854535669088\n",
      "Epoch [37/40], Batch [20], Loss: 0.005647892598062754\n",
      "Epoch [37/40], Batch [30], Loss: 0.004601883236318827\n",
      "Epoch [37/40], Batch [40], Loss: 0.005530444905161858\n",
      "Epoch [37/40], Batch [50], Loss: 0.005489988252520561\n",
      "Epoch [37/40], Average Loss: 0.006227814835159385\n",
      "Epoch [38/40], Batch [0], Loss: 0.008941348642110825\n",
      "Epoch [38/40], Batch [10], Loss: 0.005434249062091112\n",
      "Epoch [38/40], Batch [20], Loss: 0.005523711908608675\n",
      "Epoch [38/40], Batch [30], Loss: 0.004564613103866577\n",
      "Epoch [38/40], Batch [40], Loss: 0.005391713231801987\n",
      "Epoch [38/40], Batch [50], Loss: 0.005350936204195023\n",
      "Epoch [38/40], Average Loss: 0.006130338160994839\n",
      "Epoch [39/40], Batch [0], Loss: 0.008753415197134018\n",
      "Epoch [39/40], Batch [10], Loss: 0.005388687830418348\n",
      "Epoch [39/40], Batch [20], Loss: 0.005312979221343994\n",
      "Epoch [39/40], Batch [30], Loss: 0.004459143150597811\n",
      "Epoch [39/40], Batch [40], Loss: 0.005338028538972139\n",
      "Epoch [39/40], Batch [50], Loss: 0.005342650227248669\n",
      "Epoch [39/40], Average Loss: 0.006030944245372047\n",
      "Epoch [40/40], Batch [0], Loss: 0.008612783625721931\n",
      "Epoch [40/40], Batch [10], Loss: 0.0054262843914330006\n",
      "Epoch [40/40], Batch [20], Loss: 0.005220297258347273\n",
      "Epoch [40/40], Batch [30], Loss: 0.004189187660813332\n",
      "Epoch [40/40], Batch [40], Loss: 0.005040216725319624\n",
      "Epoch [40/40], Batch [50], Loss: 0.005299602169543505\n",
      "Epoch [40/40], Average Loss: 0.00590222044550675\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "from model import NETWORK  # Ensure that model.py is saved in the same directory\n",
    "from dataloaders import * # Ensure that dataloaders.py is saved in the same directory\n",
    "from utils import *\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "\n",
    "# Setup configuration\n",
    "latent_dim = 64  # Latent dimension size, can be adjusted\n",
    "hidden_dim = 512  # Hidden dimension size for the encoder and decoder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "n_components = 100\n",
    "n_knn_search = 10\n",
    "dataset_name = \"gastrulation\"\n",
    "cell_type_key = \"clusters\"\n",
    "model_name = \"VeloFormer\"\n",
    "\n",
    "num_genes = 2000\n",
    "nhead = 1 #original: 1\n",
    "embedding_dim = 128*nhead# original: 128\n",
    "num_encoder_layers = 1 #original: 1\n",
    "num_bins = 50\n",
    "batch_size = 64  # Batch size for training\n",
    "epochs = 40 # Number of epochs for training\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "lambda1 = 1e-1  # Weight for heuristic loss\n",
    "lambda2 = 1 # Weight for discrepancy loss\n",
    "K = 11  # Number of neighbors for heuristic loss\n",
    "\n",
    "# Load data\n",
    "adata = sc.read_h5ad(\"pancreas-gastr->pancr_transfer.h5ad\")\n",
    "adata = color_keys(adata, cell_type_key)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = NETWORK(input_dim=num_genes*2, latent_dim=latent_dim, \n",
    "                hidden_dim=hidden_dim, emb_dim = embedding_dim,\n",
    "                nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "                num_genes=num_genes, num_bins=num_bins).to(device)\n",
    "                \n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Reinitialize weights of the derivative_decoder and probabilities_decoder\n",
    "def reinitialize_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)         # Initialize biases to zero\n",
    "\n",
    "# Apply reinitialization to specific decoders\n",
    "model.derivative_decoder.apply(reinitialize_weights)\n",
    "model.probabilities_decoder.apply(reinitialize_weights)\n",
    "\n",
    "\n",
    "# Freeze all layers except for the derivative and probabilities decoders\n",
    "for name, param in model.named_parameters():\n",
    "    if \"derivative_decoder\" not in name and \"probabilities_decoder\" not in name:\n",
    "        param.requires_grad = False  # Freeze the parameters\n",
    "    else:\n",
    "        param.requires_grad = True   # Keep the decoders' parameters trainable\n",
    "\n",
    "\n",
    "# Ensure optimizer only updates trainable parameters\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader, test_loader, full_data_loader = setup_dataloaders_binning_simpler(adata, \n",
    "                                                                       batch_size=batch_size, \n",
    "                                                                       num_genes=num_genes,\n",
    "                                                                       num_bins=num_bins)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (tokens, data, batch_indices) in enumerate(full_data_loader):\n",
    "        tokens = tokens.to(device)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"\"\"print(tokens.shape)\n",
    "        print(data.shape)\n",
    "        print(batch_indices.shape)\"\"\"\n",
    "        \n",
    "        # Forward pass\n",
    "        out_dic = model(tokens, data)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses_dic = model.heuristic_loss(\n",
    "            adata=adata, \n",
    "            x=data, \n",
    "            batch_indices=batch_indices,\n",
    "            lambda1=lambda1, \n",
    "            lambda2=lambda2, \n",
    "            out_dic=out_dic, \n",
    "            device=device,\n",
    "            K=K\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss = losses_dic[\"total_loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for monitoring\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}], Loss: {loss.item()}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Save the model periodically\n",
    "    \"\"\"if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\"\"\"\n",
    "\n",
    "# After training, save final model\n",
    "torch.save(model.state_dict(), 'linear_probed_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTrajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
