{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import sys, os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "from model import NETWORK  # Ensure that model.py is saved in the same directory\n",
    "from dataloaders import * # Ensure that dataloaders.py is saved in the same directory\n",
    "from utils import *\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "import gc\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "latent_dim = 64  # Latent dimension size, can be adjusted\n",
    "hidden_dim = 512  # Hidden dimension size for the encoder and decoder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "n_components = 100\n",
    "n_knn_search = 10\n",
    "dataset_name = \"gastrulation_erythroid\"\n",
    "cell_type_key = \"celltype\"\n",
    "model_name = \"imVelo\"\n",
    "\n",
    "num_genes = 2000\n",
    "nhead = 1 #original: 1\n",
    "embedding_dim = 128*nhead# original: 128\n",
    "num_encoder_layers = 1 #original: 1\n",
    "num_bins = 50\n",
    "batch_size = 128  # Batch size for training\n",
    "epochs = 10  # Number of epochs for training\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "lambda1 = 1e-1  # Weight for heuristic loss\n",
    "lambda2 = 1 # Weight for discrepancy loss\n",
    "K = 11  # Number of neighbors for heuristic loss\n",
    "\n",
    "# Load data\n",
    "adata = sc.read_h5ad(\"gastrulation_erythroid_common_smoothing.h5ad\")\n",
    "\n",
    "adata.obs[cell_type_key] = [str(cat) for cat in list(adata.obs[cell_type_key])]\n",
    "adata.obs[cell_type_key] = pd.Series(adata.obs[cell_type_key], dtype=\"category\")\n",
    "unique_categories = adata.obs[cell_type_key].cat.categories\n",
    "rgb_colors = sns.color_palette(\"tab20\", len(unique_categories))\n",
    "hex_colors = ['#%02x%02x%02x' % (int(r*255), int(g*255), int(b*255)) for r, g, b in rgb_colors]\n",
    "adata.uns[f\"{cell_type_key}_colors\"] = hex_colors\n",
    "print(dataset_name)\n",
    "adata.layers['counts_unspliced'] = adata.layers[\"unspliced\"].copy()\n",
    "adata.layers['counts_spliced'] = adata.layers[\"spliced\"].copy()\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = NETWORK(input_dim=adata.shape[1]*2, latent_dim=latent_dim, \n",
    "                hidden_dim=hidden_dim, emb_dim = embedding_dim,\n",
    "                nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "                num_genes=num_genes, num_bins=num_bins).to(device)\n",
    "                \n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Ensure to call model.eval() if you're loading the model for inference to set the dropout and batch normalization layers to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, full_data_loader = setup_dataloaders_binning(adata, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    num_genes=num_genes, \n",
    "                                                    num_bins=num_bins)\n",
    "\n",
    "# Initialize empty layers in adata for storing results\n",
    "adata.layers[\"velocity_u\"] = np.zeros_like(adata.layers[\"Mu\"], dtype=np.float32)\n",
    "adata.layers[\"velocity\"] = np.zeros_like(adata.layers[\"Ms\"], dtype=np.float32)\n",
    "adata.obsm[\"pred\"] = np.zeros((adata.shape[0], adata.shape[1] * 2), dtype=np.float32)\n",
    "adata.obsm[\"cell_embeddings\"] = np.zeros((adata.shape[0], adata.shape[1] * 2), dtype=np.float32)\n",
    "adata.layers[\"pp\"] = np.zeros_like(adata.layers[\"Mu\"])  # Same shape as Mu\n",
    "adata.layers[\"nn\"] = np.zeros_like(adata.layers[\"Mu\"])  # Same shape as Mu\n",
    "adata.layers[\"pn\"] = np.zeros_like(adata.layers[\"Mu\"])  # Same shape as Mu\n",
    "adata.layers[\"np\"] = np.zeros_like(adata.layers[\"Mu\"])  # Same shape as Mu\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (tokens, data, batch_indices) in enumerate(full_data_loader):\n",
    "        print(f\"Batch {batch_idx+1}/{len(full_data_loader)}\")\n",
    "        tokens = tokens.to(device)\n",
    "        data = data.to(device)\n",
    "        out_dic = model(tokens, data)\n",
    "\n",
    "        # Store results and convert to numpy inside the loop to reduce peak memory usage\n",
    "        adata.layers[\"velocity_u\"][batch_indices] = out_dic[\"v_u\"].detach().cpu().numpy()\n",
    "        adata.layers[\"velocity\"][batch_indices] = out_dic[\"v_s\"].detach().cpu().numpy()\n",
    "        adata.obsm[\"pred\"][batch_indices] = out_dic[\"pred\"].detach().cpu().numpy()\n",
    "        adata.obsm[\"cell_embeddings\"][batch_indices] = out_dic[\"cell_embeddings\"].detach().cpu().numpy()\n",
    "        adata.layers[\"pp\"][batch_indices] = out_dic[\"pp\"].cpu().numpy()\n",
    "        adata.layers[\"nn\"][batch_indices] = out_dic[\"nn\"].cpu().numpy()\n",
    "        adata.layers[\"pn\"][batch_indices] = out_dic[\"pn\"].cpu().numpy()\n",
    "        adata.layers[\"np\"][batch_indices] = out_dic[\"np\"].cpu().numpy()\n",
    "\n",
    "        # Explicit memory cleanup\n",
    "        del tokens, data, out_dic\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  # If using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(adata.layers[\"velocity_u\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(adata.layers[\"velocity\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"MuMs\"] = np.concatenate([adata.layers[\"Mu\"], adata.layers[\"Ms\"]], axis=1)\n",
    "adata.obsm[\"velocity\"] = np.concatenate([adata.layers[\"velocity_u\"], adata.layers[\"velocity\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"velocity_u\"] *= -1\n",
    "adata.layers[\"velocity\"] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata)\n",
    "scv.tl.velocity_graph(adata)\n",
    "scv.tl.velocity_confidence(adata)\n",
    "scv.tl.velocity_pseudotime(adata)\n",
    "keys = [\"velocity_confidence\", \"velocity_length\", \"velocity_pseudotime\"]\n",
    "sc.pl.umap(adata, color=keys)\n",
    "scv.pl.velocity_embedding_stream(adata, color=keys, basis=\"umap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata,use_rep=\"velocity\")\n",
    "sc.tl.umap(adata)\n",
    "keys = [cell_type_key, \"stage\"]\n",
    "sc.pl.umap(adata, color=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"cell_embeddings\")\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color=[cell_type_key, \"stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.tl.rank_velocity_genes(adata, groupby=cell_type_key)\n",
    "pd.DataFrame(adata.uns[\"rank_velocity_genes\"][\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = [\"Actb\", \"Hba-x\", \"Rap1b\", \"Rpl18a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in adata.obs[\"stage\"].unique():\n",
    "    adata_tmp = adata[adata.obs[\"stage\"] == stage].copy()\n",
    "    sc.pp.neighbors(adata_tmp)\n",
    "    sc.tl.umap(adata_tmp)\n",
    "    scv.pl.velocity_embedding_stream(adata_tmp, color=[cell_type_key], basis=\"umap\")\n",
    "    for gene_name in gene_names:\n",
    "        plot_phase_plane(adata_tmp, gene_name, dataset_name, 11, \n",
    "                    u_scale=0.1, s_scale=0.1, cell_type_key=cell_type_key,\n",
    "                    save_path=\"plots/plot1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene_name in gene_names:\n",
    "    plot_phase_plane(adata, gene_name, dataset_name, 11, \n",
    "                    u_scale=0.1, s_scale=0.1, cell_type_key=cell_type_key,\n",
    "                    save_path=\"plots/plot1.png\")\n",
    "    plot_phase_plane(adata, gene_name, dataset_name, 11, \n",
    "                    u_scale=0.1, s_scale=0.1, cell_type_key=\"stage\",\n",
    "                    save_path=\"plots/plot1.png\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTrajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
