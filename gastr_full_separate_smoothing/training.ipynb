{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Batch [0], Loss: 0.11655070632696152\n",
      "Epoch [1/1], Batch [10], Loss: 0.08587443828582764\n",
      "Epoch [1/1], Batch [20], Loss: 0.06725024431943893\n",
      "Epoch [1/1], Batch [30], Loss: 0.04546913504600525\n",
      "Epoch [1/1], Batch [40], Loss: 0.07347115129232407\n",
      "Epoch [1/1], Batch [50], Loss: 0.04695751890540123\n",
      "Epoch [1/1], Batch [60], Loss: 0.044837117195129395\n",
      "Epoch [1/1], Batch [70], Loss: 0.06530892848968506\n",
      "Epoch [1/1], Batch [80], Loss: 0.05635891854763031\n",
      "Epoch [1/1], Batch [90], Loss: 0.05474396422505379\n",
      "Epoch [1/1], Batch [100], Loss: 0.06033838540315628\n",
      "Epoch [1/1], Batch [110], Loss: 0.043794821947813034\n",
      "Epoch [1/1], Batch [120], Loss: 0.05035460740327835\n",
      "Epoch [1/1], Batch [130], Loss: 0.045936789363622665\n",
      "Epoch [1/1], Batch [140], Loss: 0.047553952783346176\n",
      "Epoch [1/1], Batch [150], Loss: 0.04587051272392273\n",
      "Epoch [1/1], Batch [160], Loss: 0.03747443109750748\n",
      "Epoch [1/1], Batch [170], Loss: 0.032691504806280136\n",
      "Epoch [1/1], Batch [180], Loss: 0.022634292021393776\n",
      "Epoch [1/1], Batch [190], Loss: 0.0632077157497406\n",
      "Epoch [1/1], Batch [200], Loss: 0.04876585304737091\n",
      "Epoch [1/1], Batch [210], Loss: 0.02941950596868992\n",
      "Epoch [1/1], Batch [220], Loss: 0.030351880937814713\n",
      "Epoch [1/1], Batch [230], Loss: 0.021765677258372307\n",
      "Epoch [1/1], Batch [240], Loss: 0.026469292119145393\n",
      "Epoch [1/1], Batch [250], Loss: 0.016969136893749237\n",
      "Epoch [1/1], Batch [260], Loss: 0.03358432278037071\n",
      "Epoch [1/1], Batch [270], Loss: 0.02560226432979107\n",
      "Epoch [1/1], Batch [280], Loss: 0.014520187862217426\n",
      "Epoch [1/1], Batch [290], Loss: 0.01437217928469181\n",
      "Epoch [1/1], Batch [300], Loss: 0.022439995780587196\n",
      "Epoch [1/1], Batch [310], Loss: 0.02113967202603817\n",
      "Epoch [1/1], Batch [320], Loss: 0.052115049213171005\n",
      "Epoch [1/1], Batch [330], Loss: 0.04364430531859398\n",
      "Epoch [1/1], Batch [340], Loss: 0.03576500713825226\n",
      "Epoch [1/1], Batch [350], Loss: 0.04246220365166664\n",
      "Epoch [1/1], Batch [360], Loss: 0.026770632714033127\n",
      "Epoch [1/1], Batch [370], Loss: 0.030537009239196777\n",
      "Epoch [1/1], Batch [380], Loss: 0.02519121952354908\n",
      "Epoch [1/1], Batch [390], Loss: 0.04892794042825699\n",
      "Epoch [1/1], Batch [400], Loss: 0.03725004568696022\n",
      "Epoch [1/1], Batch [410], Loss: 0.02717331051826477\n",
      "Epoch [1/1], Batch [420], Loss: 0.029396507889032364\n",
      "Epoch [1/1], Batch [430], Loss: 0.025510519742965698\n",
      "Epoch [1/1], Batch [440], Loss: 0.049687448889017105\n",
      "Epoch [1/1], Batch [450], Loss: 0.03295579552650452\n",
      "Epoch [1/1], Batch [460], Loss: 0.02088882215321064\n",
      "Epoch [1/1], Batch [470], Loss: 0.023841476067900658\n",
      "Epoch [1/1], Batch [480], Loss: 0.03221697732806206\n",
      "Epoch [1/1], Batch [490], Loss: 0.03494253009557724\n",
      "Epoch [1/1], Batch [500], Loss: 0.03457801789045334\n",
      "Epoch [1/1], Batch [510], Loss: 0.03688465431332588\n",
      "Epoch [1/1], Batch [520], Loss: 0.03511543571949005\n",
      "Epoch [1/1], Batch [530], Loss: 0.027435600757598877\n",
      "Epoch [1/1], Batch [540], Loss: 0.023679619655013084\n",
      "Epoch [1/1], Batch [550], Loss: 0.030260855332016945\n",
      "Epoch [1/1], Batch [560], Loss: 0.031030643731355667\n",
      "Epoch [1/1], Batch [570], Loss: 0.0227687805891037\n",
      "Epoch [1/1], Batch [580], Loss: 0.022867461666464806\n",
      "Epoch [1/1], Batch [590], Loss: 0.028830351307988167\n",
      "Epoch [1/1], Batch [600], Loss: 0.020622989162802696\n",
      "Epoch [1/1], Batch [610], Loss: 0.017935408279299736\n",
      "Epoch [1/1], Batch [620], Loss: 0.014437838457524776\n",
      "Epoch [1/1], Batch [630], Loss: 0.02378975600004196\n",
      "Epoch [1/1], Batch [640], Loss: 0.029758483171463013\n",
      "Epoch [1/1], Batch [650], Loss: 0.02731637842953205\n",
      "Epoch [1/1], Batch [660], Loss: 0.02345125377178192\n",
      "Epoch [1/1], Batch [670], Loss: 0.012131931260228157\n",
      "Epoch [1/1], Batch [680], Loss: 0.04007669910788536\n",
      "Epoch [1/1], Batch [690], Loss: 0.048248082399368286\n",
      "Epoch [1/1], Batch [700], Loss: 0.028275730088353157\n",
      "Epoch [1/1], Batch [710], Loss: 0.044201575219631195\n",
      "Epoch [1/1], Batch [720], Loss: 0.03445350006222725\n",
      "Epoch [1/1], Batch [730], Loss: 0.02620011381804943\n",
      "Epoch [1/1], Batch [740], Loss: 0.02378948964178562\n",
      "Epoch [1/1], Batch [750], Loss: 0.02289963699877262\n",
      "Epoch [1/1], Batch [760], Loss: 0.036307476460933685\n",
      "Epoch [1/1], Batch [770], Loss: 0.034658852964639664\n",
      "Epoch [1/1], Batch [780], Loss: 0.03521215170621872\n",
      "Epoch [1/1], Batch [790], Loss: 0.029291890561580658\n",
      "Epoch [1/1], Batch [800], Loss: 0.027563786134123802\n",
      "Epoch [1/1], Batch [810], Loss: 0.025328785181045532\n",
      "Epoch [1/1], Batch [820], Loss: 0.01617339812219143\n",
      "Epoch [1/1], Batch [830], Loss: 0.023266244679689407\n",
      "Epoch [1/1], Batch [840], Loss: 0.014949804171919823\n",
      "Epoch [1/1], Batch [850], Loss: 0.016035981476306915\n",
      "Epoch [1/1], Batch [860], Loss: 0.03480752557516098\n",
      "Epoch [1/1], Batch [870], Loss: 0.01854422688484192\n",
      "Epoch [1/1], Batch [880], Loss: 0.00815216638147831\n",
      "Epoch [1/1], Batch [890], Loss: 0.01747678779065609\n",
      "Epoch [1/1], Batch [900], Loss: 0.02087489701807499\n",
      "Epoch [1/1], Batch [910], Loss: 0.06370731443166733\n",
      "Epoch [1/1], Batch [920], Loss: 0.055740632116794586\n",
      "Epoch [1/1], Batch [930], Loss: 0.03501104190945625\n",
      "Epoch [1/1], Batch [940], Loss: 0.0312582366168499\n",
      "Epoch [1/1], Batch [950], Loss: 0.03834439069032669\n",
      "Epoch [1/1], Batch [960], Loss: 0.03359541669487953\n",
      "Epoch [1/1], Batch [970], Loss: 0.03349779546260834\n",
      "Epoch [1/1], Batch [980], Loss: 0.03095240890979767\n",
      "Epoch [1/1], Batch [990], Loss: 0.04351305589079857\n",
      "Epoch [1/1], Batch [1000], Loss: 0.03107617050409317\n",
      "Epoch [1/1], Batch [1010], Loss: 0.02965504117310047\n",
      "Epoch [1/1], Batch [1020], Loss: 0.030993808060884476\n",
      "Epoch [1/1], Batch [1030], Loss: 0.03088279254734516\n",
      "Epoch [1/1], Batch [1040], Loss: 0.020491212606430054\n",
      "Epoch [1/1], Batch [1050], Loss: 0.021668924018740654\n",
      "Epoch [1/1], Batch [1060], Loss: 0.04744190350174904\n",
      "Epoch [1/1], Batch [1070], Loss: 0.01788748987019062\n",
      "Epoch [1/1], Batch [1080], Loss: 0.020099757239222527\n",
      "Epoch [1/1], Batch [1090], Loss: 0.020774170756340027\n",
      "Epoch [1/1], Batch [1100], Loss: 0.014148701913654804\n",
      "Epoch [1/1], Batch [1110], Loss: 0.0137283094227314\n",
      "Epoch [1/1], Batch [1120], Loss: 0.017345411702990532\n",
      "Epoch [1/1], Batch [1130], Loss: 0.053088825196027756\n",
      "Epoch [1/1], Batch [1140], Loss: 0.038976553827524185\n",
      "Epoch [1/1], Batch [1150], Loss: 0.03433309122920036\n",
      "Epoch [1/1], Batch [1160], Loss: 0.04073289781808853\n",
      "Epoch [1/1], Batch [1170], Loss: 0.026677070185542107\n",
      "Epoch [1/1], Batch [1180], Loss: 0.031038986518979073\n",
      "Epoch [1/1], Batch [1190], Loss: 0.02851545624434948\n",
      "Epoch [1/1], Batch [1200], Loss: 0.02157503366470337\n",
      "Epoch [1/1], Batch [1210], Loss: 0.020193472504615784\n",
      "Epoch [1/1], Batch [1220], Loss: 0.02328885719180107\n",
      "Epoch [1/1], Batch [1230], Loss: 0.022047678008675575\n",
      "Epoch [1/1], Batch [1240], Loss: 0.02866688184440136\n",
      "Epoch [1/1], Batch [1250], Loss: 0.02173352986574173\n",
      "Epoch [1/1], Batch [1260], Loss: 0.029657846316695213\n",
      "Epoch [1/1], Batch [1270], Loss: 0.027881285175681114\n",
      "Epoch [1/1], Batch [1280], Loss: 0.02217198722064495\n",
      "Epoch [1/1], Batch [1290], Loss: 0.02033236250281334\n",
      "Epoch [1/1], Batch [1300], Loss: 0.04730473831295967\n",
      "Epoch [1/1], Batch [1310], Loss: 0.05173319950699806\n",
      "Epoch [1/1], Batch [1320], Loss: 0.025167230516672134\n",
      "Epoch [1/1], Batch [1330], Loss: 0.025372523814439774\n",
      "Epoch [1/1], Batch [1340], Loss: 0.025075331330299377\n",
      "Epoch [1/1], Batch [1350], Loss: 0.03263290598988533\n",
      "Epoch [1/1], Batch [1360], Loss: 0.026988504454493523\n",
      "Epoch [1/1], Batch [1370], Loss: 0.018120434135198593\n",
      "Epoch [1/1], Batch [1380], Loss: 0.024657879024744034\n",
      "Epoch [1/1], Batch [1390], Loss: 0.04380112513899803\n",
      "Epoch [1/1], Batch [1400], Loss: 0.03429803252220154\n",
      "Epoch [1/1], Batch [1410], Loss: 0.02726203389465809\n",
      "Epoch [1/1], Batch [1420], Loss: 0.020550817251205444\n",
      "Epoch [1/1], Batch [1430], Loss: 0.030122457072138786\n",
      "Epoch [1/1], Batch [1440], Loss: 0.02507496252655983\n",
      "Epoch [1/1], Batch [1450], Loss: 0.013608266599476337\n",
      "Epoch [1/1], Batch [1460], Loss: 0.02281917817890644\n",
      "Epoch [1/1], Batch [1470], Loss: 0.03303389996290207\n",
      "Epoch [1/1], Batch [1480], Loss: 0.015922129154205322\n",
      "Epoch [1/1], Batch [1490], Loss: 0.021831870079040527\n",
      "Epoch [1/1], Batch [1500], Loss: 0.023998305201530457\n",
      "Epoch [1/1], Batch [1510], Loss: 0.01583113893866539\n",
      "Epoch [1/1], Batch [1520], Loss: 0.02001650258898735\n",
      "Epoch [1/1], Batch [1530], Loss: 0.022640366107225418\n",
      "Epoch [1/1], Batch [1540], Loss: 0.01734759658575058\n",
      "Epoch [1/1], Batch [1550], Loss: 0.018401553854346275\n",
      "Epoch [1/1], Batch [1560], Loss: 0.016589073464274406\n",
      "Epoch [1/1], Batch [1570], Loss: 0.02778051048517227\n",
      "Epoch [1/1], Batch [1580], Loss: 0.021788910031318665\n",
      "Epoch [1/1], Batch [1590], Loss: 0.013880380429327488\n",
      "Epoch [1/1], Batch [1600], Loss: 0.017403068020939827\n",
      "Epoch [1/1], Batch [1610], Loss: 0.016966423019766808\n",
      "Epoch [1/1], Batch [1620], Loss: 0.023792225867509842\n",
      "Epoch [1/1], Batch [1630], Loss: 0.021620046347379684\n",
      "Epoch [1/1], Batch [1640], Loss: 0.022504271939396858\n",
      "Epoch [1/1], Batch [1650], Loss: 0.017239924520254135\n",
      "Epoch [1/1], Batch [1660], Loss: 0.019120672717690468\n",
      "Epoch [1/1], Batch [1670], Loss: 0.015995662659406662\n",
      "Epoch [1/1], Batch [1680], Loss: 0.019395288079977036\n",
      "Epoch [1/1], Batch [1690], Loss: 0.0222227293998003\n",
      "Epoch [1/1], Batch [1700], Loss: 0.01571771502494812\n",
      "Epoch [1/1], Batch [1710], Loss: 0.018123706802725792\n",
      "Epoch [1/1], Batch [1720], Loss: 0.022969365119934082\n",
      "Epoch [1/1], Batch [1730], Loss: 0.01358972117304802\n",
      "Epoch [1/1], Batch [1740], Loss: 0.023819351568818092\n",
      "Epoch [1/1], Batch [1750], Loss: 0.014453462325036526\n",
      "Epoch [1/1], Batch [1760], Loss: 0.0334673747420311\n",
      "Epoch [1/1], Batch [1770], Loss: 0.016560450196266174\n",
      "Epoch [1/1], Batch [1780], Loss: 0.022019704803824425\n",
      "Epoch [1/1], Batch [1790], Loss: 0.022758470848202705\n",
      "Epoch [1/1], Batch [1800], Loss: 0.0163283534348011\n",
      "Epoch [1/1], Batch [1810], Loss: 0.02990613877773285\n",
      "Epoch [1/1], Batch [1820], Loss: 0.01955932006239891\n",
      "Epoch [1/1], Batch [1830], Loss: 0.017989031970500946\n",
      "Epoch [1/1], Batch [1840], Loss: 0.010258340276777744\n",
      "Epoch [1/1], Batch [1850], Loss: 0.018767958506941795\n",
      "Epoch [1/1], Batch [1860], Loss: 0.016791297122836113\n",
      "Epoch [1/1], Batch [1870], Loss: 0.0414731539785862\n",
      "Epoch [1/1], Batch [1880], Loss: 0.032108671963214874\n",
      "Epoch [1/1], Batch [1890], Loss: 0.02778036706149578\n",
      "Epoch [1/1], Batch [1900], Loss: 0.022858180105686188\n",
      "Epoch [1/1], Batch [1910], Loss: 0.019633540883660316\n",
      "Epoch [1/1], Batch [1920], Loss: 0.02090839110314846\n",
      "Epoch [1/1], Batch [1930], Loss: 0.024742016568779945\n",
      "Epoch [1/1], Batch [1940], Loss: 0.0258500836789608\n",
      "Epoch [1/1], Batch [1950], Loss: 0.017706912010908127\n",
      "Epoch [1/1], Batch [1960], Loss: 0.018138019368052483\n",
      "Epoch [1/1], Batch [1970], Loss: 0.016946464776992798\n",
      "Epoch [1/1], Batch [1980], Loss: 0.012700841762125492\n",
      "Epoch [1/1], Batch [1990], Loss: 0.028231699019670486\n",
      "Epoch [1/1], Batch [2000], Loss: 0.014725989662110806\n",
      "Epoch [1/1], Batch [2010], Loss: 0.016865119338035583\n",
      "Epoch [1/1], Batch [2020], Loss: 0.014558179304003716\n",
      "Epoch [1/1], Batch [2030], Loss: 0.014796283096075058\n",
      "Epoch [1/1], Batch [2040], Loss: 0.013644245453178883\n",
      "Epoch [1/1], Batch [2050], Loss: 0.014007930643856525\n",
      "Epoch [1/1], Batch [2060], Loss: 0.018759459257125854\n",
      "Epoch [1/1], Batch [2070], Loss: 0.017995348200201988\n",
      "Epoch [1/1], Batch [2080], Loss: 0.013870233669877052\n",
      "Epoch [1/1], Batch [2090], Loss: 0.019616786390542984\n",
      "Epoch [1/1], Batch [2100], Loss: 0.012954561039805412\n",
      "Epoch [1/1], Batch [2110], Loss: 0.017161492258310318\n",
      "Epoch [1/1], Batch [2120], Loss: 0.01661456748843193\n",
      "Epoch [1/1], Batch [2130], Loss: 0.013377889059484005\n",
      "Epoch [1/1], Batch [2140], Loss: 0.018821604549884796\n",
      "Epoch [1/1], Batch [2150], Loss: 0.014653785154223442\n",
      "Epoch [1/1], Batch [2160], Loss: 0.021829625591635704\n",
      "Epoch [1/1], Batch [2170], Loss: 0.015264950692653656\n",
      "Epoch [1/1], Batch [2180], Loss: 0.01708962954580784\n",
      "Epoch [1/1], Batch [2190], Loss: 0.012544373981654644\n",
      "Epoch [1/1], Batch [2200], Loss: 0.01777120865881443\n",
      "Epoch [1/1], Batch [2210], Loss: 0.016474749892950058\n",
      "Epoch [1/1], Batch [2220], Loss: 0.01916421391069889\n",
      "Epoch [1/1], Batch [2230], Loss: 0.01748819649219513\n",
      "Epoch [1/1], Batch [2240], Loss: 0.013953736051917076\n",
      "Epoch [1/1], Batch [2250], Loss: 0.016647834330797195\n",
      "Epoch [1/1], Batch [2260], Loss: 0.015919605270028114\n",
      "Epoch [1/1], Batch [2270], Loss: 0.030394364148378372\n",
      "Epoch [1/1], Batch [2280], Loss: 0.0340283177793026\n",
      "Epoch [1/1], Batch [2290], Loss: 0.024249302223324776\n",
      "Epoch [1/1], Batch [2300], Loss: 0.03184635937213898\n",
      "Epoch [1/1], Batch [2310], Loss: 0.015626994892954826\n",
      "Epoch [1/1], Batch [2320], Loss: 0.020785290747880936\n",
      "Epoch [1/1], Batch [2330], Loss: 0.024315323680639267\n",
      "Epoch [1/1], Batch [2340], Loss: 0.02214011922478676\n",
      "Epoch [1/1], Batch [2350], Loss: 0.018472937867045403\n",
      "Epoch [1/1], Batch [2360], Loss: 0.030126551166176796\n",
      "Epoch [1/1], Batch [2370], Loss: 0.03425731137394905\n",
      "Epoch [1/1], Batch [2380], Loss: 0.017003949731588364\n",
      "Epoch [1/1], Batch [2390], Loss: 0.024264706298708916\n",
      "Epoch [1/1], Batch [2400], Loss: 0.018405264243483543\n",
      "Epoch [1/1], Batch [2410], Loss: 0.03136125206947327\n",
      "Epoch [1/1], Batch [2420], Loss: 0.03316598758101463\n",
      "Epoch [1/1], Batch [2430], Loss: 0.03262503445148468\n",
      "Epoch [1/1], Batch [2440], Loss: 0.03795009106397629\n",
      "Epoch [1/1], Batch [2450], Loss: 0.0305603239685297\n",
      "Epoch [1/1], Batch [2460], Loss: 0.026612412184476852\n",
      "Epoch [1/1], Batch [2470], Loss: 0.03357793018221855\n",
      "Epoch [1/1], Batch [2480], Loss: 0.027828648686408997\n",
      "Epoch [1/1], Batch [2490], Loss: 0.025475487112998962\n",
      "Epoch [1/1], Batch [2500], Loss: 0.032374173402786255\n",
      "Epoch [1/1], Batch [2510], Loss: 0.024782968685030937\n",
      "Epoch [1/1], Batch [2520], Loss: 0.02355673909187317\n",
      "Epoch [1/1], Batch [2530], Loss: 0.024636218324303627\n",
      "Epoch [1/1], Batch [2540], Loss: 0.020731600001454353\n",
      "Epoch [1/1], Batch [2550], Loss: 0.026863308623433113\n",
      "Epoch [1/1], Batch [2560], Loss: 0.02191423438489437\n",
      "Epoch [1/1], Batch [2570], Loss: 0.02240839973092079\n",
      "Epoch [1/1], Batch [2580], Loss: 0.03585867956280708\n",
      "Epoch [1/1], Batch [2590], Loss: 0.018326882272958755\n",
      "Epoch [1/1], Batch [2600], Loss: 0.029547354206442833\n",
      "Epoch [1/1], Batch [2610], Loss: 0.02614028938114643\n",
      "Epoch [1/1], Batch [2620], Loss: 0.028023691847920418\n",
      "Epoch [1/1], Batch [2630], Loss: 0.022240091115236282\n",
      "Epoch [1/1], Batch [2640], Loss: 0.028445035219192505\n",
      "Epoch [1/1], Batch [2650], Loss: 0.02189737744629383\n",
      "Epoch [1/1], Batch [2660], Loss: 0.03756093978881836\n",
      "Epoch [1/1], Batch [2670], Loss: 0.025129325687885284\n",
      "Epoch [1/1], Batch [2680], Loss: 0.021191230043768883\n",
      "Epoch [1/1], Batch [2690], Loss: 0.02119281329214573\n",
      "Epoch [1/1], Batch [2700], Loss: 0.022416219115257263\n",
      "Epoch [1/1], Batch [2710], Loss: 0.017010224983096123\n",
      "Epoch [1/1], Batch [2720], Loss: 0.022999990731477737\n",
      "Epoch [1/1], Batch [2730], Loss: 0.013521309942007065\n",
      "Epoch [1/1], Batch [2740], Loss: 0.019660675898194313\n",
      "Epoch [1/1], Batch [2750], Loss: 0.011096712201833725\n",
      "Epoch [1/1], Batch [2760], Loss: 0.013665769249200821\n",
      "Epoch [1/1], Batch [2770], Loss: 0.014900616370141506\n",
      "Epoch [1/1], Batch [2780], Loss: 0.014110444113612175\n",
      "Epoch [1/1], Batch [2790], Loss: 0.02232101932168007\n",
      "Epoch [1/1], Batch [2800], Loss: 0.00998709350824356\n",
      "Epoch [1/1], Batch [2810], Loss: 0.0083349933847785\n",
      "Epoch [1/1], Batch [2820], Loss: 0.01792660914361477\n",
      "Epoch [1/1], Batch [2830], Loss: 0.014641253277659416\n",
      "Epoch [1/1], Batch [2840], Loss: 0.012800105847418308\n",
      "Epoch [1/1], Batch [2850], Loss: 0.008419874124228954\n",
      "Epoch [1/1], Batch [2860], Loss: 0.008880461566150188\n",
      "Epoch [1/1], Batch [2870], Loss: 0.009806007146835327\n",
      "Epoch [1/1], Batch [2880], Loss: 0.007581263780593872\n",
      "Epoch [1/1], Batch [2890], Loss: 0.017872275784611702\n",
      "Epoch [1/1], Batch [2900], Loss: 0.017889032140374184\n",
      "Epoch [1/1], Batch [2910], Loss: 0.012417343445122242\n",
      "Epoch [1/1], Batch [2920], Loss: 0.011059751734137535\n",
      "Epoch [1/1], Batch [2930], Loss: 0.016453176736831665\n",
      "Epoch [1/1], Batch [2940], Loss: 0.009594975970685482\n",
      "Epoch [1/1], Batch [2950], Loss: 0.01332024298608303\n",
      "Epoch [1/1], Batch [2960], Loss: 0.010310460813343525\n",
      "Epoch [1/1], Batch [2970], Loss: 0.011365069076418877\n",
      "Epoch [1/1], Batch [2980], Loss: 0.007635754998773336\n",
      "Epoch [1/1], Batch [2990], Loss: 0.013562063686549664\n",
      "Epoch [1/1], Batch [3000], Loss: 0.036049120128154755\n",
      "Epoch [1/1], Batch [3010], Loss: 0.021740419790148735\n",
      "Epoch [1/1], Batch [3020], Loss: 0.025508463382720947\n",
      "Epoch [1/1], Batch [3030], Loss: 0.019861463457345963\n",
      "Epoch [1/1], Batch [3040], Loss: 0.039268046617507935\n",
      "Epoch [1/1], Batch [3050], Loss: 0.014650377444922924\n",
      "Epoch [1/1], Batch [3060], Loss: 0.024306803941726685\n",
      "Epoch [1/1], Batch [3070], Loss: 0.029131585732102394\n",
      "Epoch [1/1], Batch [3080], Loss: 0.026467956602573395\n",
      "Epoch [1/1], Batch [3090], Loss: 0.022069983184337616\n",
      "Epoch [1/1], Batch [3100], Loss: 0.02127070166170597\n",
      "Epoch [1/1], Batch [3110], Loss: 0.027361487969756126\n",
      "Epoch [1/1], Batch [3120], Loss: 0.01439866703003645\n",
      "Epoch [1/1], Batch [3130], Loss: 0.014764349907636642\n",
      "Epoch [1/1], Batch [3140], Loss: 0.007331270258873701\n",
      "Epoch [1/1], Batch [3150], Loss: 0.020467601716518402\n",
      "Epoch [1/1], Batch [3160], Loss: 0.029914211481809616\n",
      "Epoch [1/1], Batch [3170], Loss: 0.016482502222061157\n",
      "Epoch [1/1], Batch [3180], Loss: 0.013379647396504879\n",
      "Epoch [1/1], Batch [3190], Loss: 0.010699194855988026\n",
      "Epoch [1/1], Batch [3200], Loss: 0.019068801775574684\n",
      "Epoch [1/1], Batch [3210], Loss: 0.013886645436286926\n",
      "Epoch [1/1], Batch [3220], Loss: 0.03329785168170929\n",
      "Epoch [1/1], Batch [3230], Loss: 0.02185598574578762\n",
      "Epoch [1/1], Batch [3240], Loss: 0.013647942803800106\n",
      "Epoch [1/1], Batch [3250], Loss: 0.01596173830330372\n",
      "Epoch [1/1], Batch [3260], Loss: 0.025937087833881378\n",
      "Epoch [1/1], Batch [3270], Loss: 0.0284423865377903\n",
      "Epoch [1/1], Batch [3280], Loss: 0.009867973625659943\n",
      "Epoch [1/1], Batch [3290], Loss: 0.022980492562055588\n",
      "Epoch [1/1], Batch [3300], Loss: 0.021597106009721756\n",
      "Epoch [1/1], Batch [3310], Loss: 0.015410196036100388\n",
      "Epoch [1/1], Batch [3320], Loss: 0.030113372951745987\n",
      "Epoch [1/1], Batch [3330], Loss: 0.025900956243276596\n",
      "Epoch [1/1], Batch [3340], Loss: 0.03070991300046444\n",
      "Epoch [1/1], Batch [3350], Loss: 0.019422603771090508\n",
      "Epoch [1/1], Batch [3360], Loss: 0.018117263913154602\n",
      "Epoch [1/1], Batch [3370], Loss: 0.014079486951231956\n",
      "Epoch [1/1], Batch [3380], Loss: 0.019304733723402023\n",
      "Epoch [1/1], Batch [3390], Loss: 0.015742700546979904\n",
      "Epoch [1/1], Batch [3400], Loss: 0.01538399513810873\n",
      "Epoch [1/1], Batch [3410], Loss: 0.013862214051187038\n",
      "Epoch [1/1], Batch [3420], Loss: 0.013029580004513264\n",
      "Epoch [1/1], Batch [3430], Loss: 0.010754336602985859\n",
      "Epoch [1/1], Batch [3440], Loss: 0.015317531302571297\n",
      "Epoch [1/1], Batch [3450], Loss: 0.03779773414134979\n",
      "Epoch [1/1], Batch [3460], Loss: 0.025670837610960007\n",
      "Epoch [1/1], Batch [3470], Loss: 0.034318957477808\n",
      "Epoch [1/1], Batch [3480], Loss: 0.030562251806259155\n",
      "Epoch [1/1], Batch [3490], Loss: 0.024877361953258514\n",
      "Epoch [1/1], Batch [3500], Loss: 0.02459367923438549\n",
      "Epoch [1/1], Batch [3510], Loss: 0.02393454499542713\n",
      "Epoch [1/1], Batch [3520], Loss: 0.016923995688557625\n",
      "Epoch [1/1], Batch [3530], Loss: 0.03289860486984253\n",
      "Epoch [1/1], Batch [3540], Loss: 0.023134609684348106\n",
      "Epoch [1/1], Batch [3550], Loss: 0.03720587119460106\n",
      "Epoch [1/1], Batch [3560], Loss: 0.02789914794266224\n",
      "Epoch [1/1], Batch [3570], Loss: 0.0304213035851717\n",
      "Epoch [1/1], Batch [3580], Loss: 0.02600025199353695\n",
      "Epoch [1/1], Batch [3590], Loss: 0.01930059865117073\n",
      "Epoch [1/1], Batch [3600], Loss: 0.026385150849819183\n",
      "Epoch [1/1], Batch [3610], Loss: 0.03051765263080597\n",
      "Epoch [1/1], Batch [3620], Loss: 0.02280455082654953\n",
      "Epoch [1/1], Batch [3630], Loss: 0.028079019859433174\n",
      "Epoch [1/1], Batch [3640], Loss: 0.019541535526514053\n",
      "Epoch [1/1], Batch [3650], Loss: 0.022190062329173088\n",
      "Epoch [1/1], Batch [3660], Loss: 0.027310103178024292\n",
      "Epoch [1/1], Batch [3670], Loss: 0.028491543605923653\n",
      "Epoch [1/1], Batch [3680], Loss: 0.01824279874563217\n",
      "Epoch [1/1], Batch [3690], Loss: 0.014930383302271366\n",
      "Epoch [1/1], Batch [3700], Loss: 0.024184340611100197\n",
      "Epoch [1/1], Batch [3710], Loss: 0.018577955663204193\n",
      "Epoch [1/1], Average Loss: 0.03087288970412481\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "from model import NETWORK  # Ensure that model.py is saved in the same directory\n",
    "from dataloaders import * # Ensure that dataloaders.py is saved in the same directory\n",
    "from utils import *\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "\n",
    "# Setup configuration\n",
    "latent_dim = 64  # Latent dimension size, can be adjusted\n",
    "hidden_dim = 512  # Hidden dimension size for the encoder and decoder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "n_components = 100\n",
    "n_knn_search = 10\n",
    "dataset_name = \"gastrulation\"\n",
    "cell_type_key = \"celltype\"\n",
    "model_name = \"VeloFormer\"\n",
    "\n",
    "num_genes = 2000\n",
    "nhead = 1 #original: 1\n",
    "embedding_dim = 128*nhead# original: 128\n",
    "num_encoder_layers = 1 #original: 1\n",
    "num_bins = 50\n",
    "batch_size = 24  # Batch size for training\n",
    "epochs = 1 # Number of epochs for training\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "lambda1 = 1e-1  # Weight for heuristic loss\n",
    "lambda2 = 1 # Weight for discrepancy loss\n",
    "K = 11  # Number of neighbors for heuristic loss\n",
    "\n",
    "# Load data\n",
    "adata = sc.read_h5ad(\"gastrulation_processed.h5ad\")\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = NETWORK(input_dim=adata.shape[1]*2, latent_dim=latent_dim, \n",
    "                hidden_dim=hidden_dim, emb_dim = embedding_dim,\n",
    "                nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "                num_genes=adata.shape[1], num_bins=num_bins).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader, test_loader, full_data_loader = setup_dataloaders_binning(adata, \n",
    "                                                                       batch_size=batch_size, \n",
    "                                                                       num_genes=num_genes,\n",
    "                                                                       num_bins=num_bins)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (tokens, data, batch_indices) in enumerate(full_data_loader):\n",
    "        tokens = tokens.to(device)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"\"\"print(tokens.shape)\n",
    "        print(data.shape)\n",
    "        print(batch_indices.shape)\"\"\"\n",
    "        \n",
    "        # Forward pass\n",
    "        out_dic = model(tokens, data)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses_dic = model.heuristic_loss(\n",
    "            adata=adata, \n",
    "            x=data, \n",
    "            batch_indices=batch_indices,\n",
    "            lambda1=lambda1, \n",
    "            lambda2=lambda2, \n",
    "            out_dic=out_dic, \n",
    "            device=device,\n",
    "            K=K\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss = losses_dic[\"total_loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for monitoring\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}], Loss: {loss.item()}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Save the model periodically\n",
    "    \"\"\"if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\"\"\"\n",
    "\n",
    "# After training, save final model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Epiblast',\n",
       " 'Primitive Streak',\n",
       " 'Visceral endoderm',\n",
       " 'Nascent mesoderm',\n",
       " 'Rostral neurectoderm',\n",
       " 'Blood progenitors 2',\n",
       " 'Mixed mesoderm',\n",
       " 'ExE mesoderm',\n",
       " 'Intermediate mesoderm',\n",
       " 'Pharyngeal mesoderm',\n",
       " 'Caudal epiblast',\n",
       " 'PGC',\n",
       " 'Mesenchyme',\n",
       " 'Haematoendothelial progenitors',\n",
       " 'Blood progenitors 1',\n",
       " 'Surface ectoderm',\n",
       " 'Gut',\n",
       " 'Paraxial mesoderm',\n",
       " 'Caudal neurectoderm',\n",
       " 'Notochord',\n",
       " 'Somitic mesoderm',\n",
       " 'Caudal Mesoderm',\n",
       " 'Erythroid1',\n",
       " 'Def. endoderm',\n",
       " 'Allantois',\n",
       " 'Anterior Primitive Streak',\n",
       " 'Endothelium',\n",
       " 'Forebrain/Midbrain/Hindbrain',\n",
       " 'Spinal cord',\n",
       " 'Cardiomyocytes',\n",
       " 'Erythroid2',\n",
       " 'NMP',\n",
       " 'Erythroid3',\n",
       " 'Neural crest']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(adata.obs[\"celltype\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "cell_1         E6.5\n",
       "cell_2         E6.5\n",
       "cell_6         E6.5\n",
       "cell_8         E6.5\n",
       "cell_9         E6.5\n",
       "               ... \n",
       "cell_139326    E8.5\n",
       "cell_139327    E8.5\n",
       "cell_139329    E8.5\n",
       "cell_139330    E8.5\n",
       "cell_139331    E8.5\n",
       "Name: stage, Length: 89267, dtype: category\n",
       "Categories (9, object): ['E6.5', 'E6.75', 'E7.0', 'E7.25', ..., 'E7.75', 'E8.0', 'E8.25', 'E8.5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs[\"stage\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTrajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
